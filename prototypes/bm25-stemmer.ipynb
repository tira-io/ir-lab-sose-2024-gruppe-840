{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "ensure_pyterrier_is_loaded()\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from tqdm import tqdm\n",
    "from jnius import autoclass\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "tira = Client()\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset and the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "dataset = 'ir-acl-anthology-20240504-training'\n",
    "pt_dataset = pt.get_dataset(f'irds:ir-lab-sose-2024/{dataset}')\n",
    "\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmer and Lemmatizer  (funktioniert leider nicht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variante 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def stem(t):\\n    lemmatizer = WordNetLemmatizer()\\n    return lemmatizer.lemmatize(t)\\n\\nlemmatizer = WordNetLemmatizer()\\n\\n\\nindexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=lemmatizer)\\nindex_ref = indexer.index(pt_dataset.get_corpus_iter())\\nindex = pt.IndexFactory.of(index_ref)\\n\\nbm25_stem = pt.BatchRetrieve(index, wmodel=\"BM25\")\\n\\n\\n\\ndef stem(t):\\n    stemmer = LancasterStemmer()\\n    return stemmer.stem(t)\\n\\nstemmer = LancasterStemmer()\\n\\nindexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=stemmer)\\nindex_ref = indexer.index(pt_dataset.get_corpus_iter())\\nindex = pt.IndexFactory.of(index_ref)\\n\\n\\nbm25_lem = pt.BatchRetrieve(index, wmodel=\"BM25\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def stem(t):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(t)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=lemmatizer)\n",
    "index_ref = indexer.index(pt_dataset.get_corpus_iter())\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "bm25_stem = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "\n",
    "\n",
    "def stem(t):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return stemmer.stem(t)\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=stemmer)\n",
    "index_ref = indexer.index(pt_dataset.get_corpus_iter())\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "\n",
    "bm25_lem = pt.BatchRetrieve(index, wmodel=\"BM25\")'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variante 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def stem_text(text):\\n    stemmer = LancasterStemmer()\\n    return \" \".join([stemmer.stem(word) for word in nltk.word_tokenize(text)])\\n\\ndef lemmatize_text(text):\\n    lemmatizer = WordNetLemmatizer()\\n    return \" \".join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\\n\\ndef document_iterator(docs, transform_func):\\n    for doc in docs:\\n        doc[\"text\"] = transform_func(doc[\"text\"])\\n        yield doc\\n\\n\\ntransform_func = stem_text  # oder lemmatize_text\\n\\n\\nindexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True)\\nindex_ref = indexer.index(document_iterator(pt_dataset.get_corpus_iter(), transform_func))\\n\\nclass NLTKStemmerTransformer(pt.Transformer):\\n    def __init__(self, stemmer):\\n        self.stemmer = stemmer\\n\\n    def transform(self, queries):\\n        queries[\\'query\\'] = queries[\\'query\\'].apply(stem_text)\\n        return queries\\n\\n\\nstemmer = LancasterStemmer()\\nnltk_stemmer_transformer = NLTKStemmerTransformer(stemmer)\\n\\n\\nindex = pt.IndexFactory.of(index_ref)\\n\\n\\nbm25_stem = nltk_stemmer_transformer >> pt.BatchRetrieve(index, wmodel=\"BM25\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def stem_text(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in nltk.word_tokenize(text)])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\n",
    "\n",
    "def document_iterator(docs, transform_func):\n",
    "    for doc in docs:\n",
    "        doc[\"text\"] = transform_func(doc[\"text\"])\n",
    "        yield doc\n",
    "\n",
    "\n",
    "transform_func = stem_text  # oder lemmatize_text\n",
    "\n",
    "\n",
    "indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True)\n",
    "index_ref = indexer.index(document_iterator(pt_dataset.get_corpus_iter(), transform_func))\n",
    "\n",
    "class NLTKStemmerTransformer(pt.Transformer):\n",
    "    def __init__(self, stemmer):\n",
    "        self.stemmer = stemmer\n",
    "\n",
    "    def transform(self, queries):\n",
    "        queries['query'] = queries['query'].apply(stem_text)\n",
    "        return queries\n",
    "\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "nltk_stemmer_transformer = NLTKStemmerTransformer(stemmer)\n",
    "\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "\n",
    "bm25_stem = nltk_stemmer_transformer >> pt.BatchRetrieve(index, wmodel=\"BM25\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npt.Experiment(\\n    retr_systems=[bm25, bm25_stem],\\n    topics=pt_dataset.get_topics('text'),\\n    qrels=pt_dataset.get_qrels(),\\n    names=['BM25', 'bm25_stem'],\\n    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\\n)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pt.Experiment(\n",
    "    retr_systems=[bm25, bm25_stem],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25', 'bm25_stem'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\n",
    ")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider kommt immer wieder 0.0 raus egal welche Variante ich benutze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Expansion by Query Prediction with docT5query\n",
    "The basic idea is to train a model, that when given an input document, generates questions that the document might answer (or more broadly, queries for which the document might be relevant). These predicted questions (or queries) are then appended to the original documents, which are then indexed as before. The docT5query model gets its name from the use of T5 as the expansion model.\n",
    "\n",
    "The primary advantage of this approach is that expensive neural inference is pushed to indexing time, which means that \"bag of words\" queries against an inverted index built on the augmented document collection are only slightly slower (due to longer documents) — but the retrieval results are much better.\n",
    "\n",
    "First we check, if our corpus has a high recall or a lower. Our Corpus in this case is the union of the IR Anthology and the ACL Anthology. The recall may change if we use another corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\\n\\npt.Experiment(\\n    retr_systems=[bm25],\\n    topics=pt_dataset.get_topics(\\'text\\'),\\n    qrels=pt_dataset.get_qrels(),\\n    names=[\\'BM25\\'],\\n    eval_metrics=[\\'recall_1000\\']\\n)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "pt.Experiment(\n",
    "    retr_systems=[bm25],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25'],\n",
    "    eval_metrics=['recall_1000']\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have already a high recall. This is important for the way we implement the docT5query.\n",
    "More information about the implementation in the [Tutorial](https://github.com/tira-io/teaching-ir-with-shared-tasks/blob/main/tutorials/tutorial-doc-t5-query.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def doc_t5_query(dataset):\\n    docs = tira.get_run_output(\\'ir-benchmarks/seanmacavaney/DocT5Query\\', dataset) + \\'/documents.jsonl.gz\\'\\n    with gzip.open(docs, \\'rt\\') as f:\\n        for l in tqdm(f):\\n            l = json.loads(l)\\n            l[\\'text\\'] = l[\\'querygen\\']\\n            l[\\'docno\\'] = l[\\'doc_id\\']\\n            del l[\\'doc_id\\']\\n            del l[\\'querygen\\']\\n            yield l\\n\\ndef doc_t5_query_index(dataset):\\n    indexer = pt.IterDictIndexer(\"/tmp/index2\", overwrite=True, meta={\\'docno\\': 100, \\'text\\': 20480})\\n    index_ref = indexer.index(doc_t5_query(dataset))\\n    return pt.IndexFactory.of(index_ref)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def doc_t5_query(dataset):\n",
    "    docs = tira.get_run_output('ir-benchmarks/seanmacavaney/DocT5Query', dataset) + '/documents.jsonl.gz'\n",
    "    with gzip.open(docs, 'rt') as f:\n",
    "        for l in tqdm(f):\n",
    "            l = json.loads(l)\n",
    "            l['text'] = l['querygen']\n",
    "            l['docno'] = l['doc_id']\n",
    "            del l['doc_id']\n",
    "            del l['querygen']\n",
    "            yield l\n",
    "\n",
    "def doc_t5_query_index(dataset):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index2\", overwrite=True, meta={'docno': 100, 'text': 20480})\n",
    "    index_ref = indexer.index(doc_t5_query(dataset))\n",
    "    return pt.IndexFactory.of(index_ref)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexD = doc_t5_query_index(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"docs_retrieved_by_bm25 = {}\\n\\nbm25_result = bm25(pt_dataset.get_topics('title'))\\n\\nfor _, i in tqdm(bm25_result.iterrows()):\\n    qid, docno = str(i['qid']), str(i['docno'])\\n\\n    if qid not in docs_retrieved_by_bm25:\\n        docs_retrieved_by_bm25[qid] = set()\\n    \\n    docs_retrieved_by_bm25[qid].add(docno)\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''docs_retrieved_by_bm25 = {}\n",
    "\n",
    "bm25_result = bm25(pt_dataset.get_topics('title'))\n",
    "\n",
    "for _, i in tqdm(bm25_result.iterrows()):\n",
    "    qid, docno = str(i['qid']), str(i['docno'])\n",
    "\n",
    "    if qid not in docs_retrieved_by_bm25:\n",
    "        docs_retrieved_by_bm25[qid] = set()\n",
    "    \n",
    "    docs_retrieved_by_bm25[qid].add(docno)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"omit_already_retrieved_docs = lambda i: i[i.apply(lambda j: str(j['docno']) not in docs_retrieved_by_bm25[str(j['qid'])], axis=1)]\\nomit_already_retrieved_docs = pt.apply.generic(omit_already_retrieved_docs)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''omit_already_retrieved_docs = lambda i: i[i.apply(lambda j: str(j['docno']) not in docs_retrieved_by_bm25[str(j['qid'])], axis=1)]\n",
    "omit_already_retrieved_docs = pt.apply.generic(omit_already_retrieved_docs)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bm25_doct5query = pt.BatchRetrieve(indexD, wmodel=\"BM25\")\\nbm25_doct5query_new = bm25_doct5query >> omit_already_retrieved_docs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bm25_doct5query = pt.BatchRetrieve(indexD, wmodel=\"BM25\")\n",
    "bm25_doct5query_new = bm25_doct5query >> omit_already_retrieved_docs'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DocT5Query bringt uns im normalen dataset nicht so viel, also lassen wir es weg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:  71%|███████   | 90198/126958 [00:18<00:06, 6118.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:38:26.135 [ForkJoinPool-2-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (2020.mir_conference-2020.1) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:24<00:00, 5238.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:38:34.169 [ForkJoinPool-2-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 3 empty documents\n"
     ]
    }
   ],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "def create_index(documents, stopwords):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=customStopwords)\n",
    "    index_ref = indexer.index(documents)\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "\n",
    "customStopwords =[\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', \n",
    "    'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "    'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', \n",
    "    'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \n",
    "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
    "    'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', \n",
    "    'will', 'just', 'don', 'should', 'now', 'e', 'n', 'd', 'o', 'c', 'r'\n",
    "]\n",
    "\n",
    "index = create_index(pt_dataset.get_corpus_iter(), customStopwords)\n",
    "\n",
    "bm25_stopwords = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion with Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure BM25 is initialized\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "bm25_rm3 = bm25 >> pt.rewrite.RM3(index) >> bm25\n",
    "bm25_kl = bm25 >> pt.rewrite.KLQueryExpansion(index) >> bm25\n",
    "\n",
    "# llm expansions with gpt\n",
    "gpt_cot = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-cot', dataset, prefix='llm_expansion_')\n",
    "gpt_sq_fs = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-sq-fs', dataset, prefix='llm_expansion_')\n",
    "gpt_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-gpt3.5-sq-zs', dataset, prefix='llm_expansion_')\n",
    "\n",
    "# llm expansions with llama\n",
    "llama_cot = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-cot', dataset, prefix='llm_expansion_')\n",
    "llama_sq_fs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-fs', dataset, prefix='llm_expansion_')\n",
    "llama_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-zs', dataset, prefix='llm_expansion_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 56287.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tokeniser = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "\n",
    "documents = []\n",
    "for doc in pt_dataset.get_corpus_iter():\n",
    "        documents.append({\n",
    "        'docno': doc['docno'],\n",
    "        'text': doc['text'],\n",
    "})   \n",
    "\n",
    "def pt_tokenize(text):\n",
    "    return ' '.join(tokeniser.getTokens(text))\n",
    "\n",
    "# we wrap this into an pyterrier transformer\n",
    "# Documentation: https://pyterrier.readthedocs.io/en/latest/apply.html\n",
    "\n",
    "def calculate_tf_idf(term, doc_text, index):\n",
    "    # Implement your TF-IDF calculation logic here\n",
    "    # Example placeholder implementation:\n",
    "    tf = doc_text.lower().count(term.lower()) / len(doc_text.split())\n",
    "    #print(tf)\n",
    "    num_docs = index.getCollectionStatistics().getNumberOfDocuments()\n",
    "    lexicon = index.getLexicon()\n",
    "    entry = lexicon.getLexiconEntry(term.lower())\n",
    "    doc_freq = entry.getDocumentFrequency() if entry is not None else 0\n",
    "    idf = math.log((num_docs + 1) / (doc_freq + 1)) + 1\n",
    "\n",
    "    #print(tf * idf)\n",
    "    return tf * idf\n",
    "\n",
    "# Define a function to retrieve top documents and extract terms with scores\n",
    "def retrieve_top_docs_terms(query, num_docs=10, num_terms=5):\n",
    "    # Annahme: bm25 ist ein BatchRetrieve-Objekt\n",
    "    from jnius import autoclass\n",
    "    tokeniser = autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "    query = \" \".join(tokeniser.getTokens(query))\n",
    "    query_results = bm25.search(query)\n",
    "    top_docs = query_results.head(num_docs)['docid'].tolist()\n",
    "    terms_scores = {}\n",
    "    \n",
    "    for docid in top_docs:\n",
    "        \n",
    "        doc_text = documents[docid]['text']\n",
    "        doc_terms = pt_tokenize(doc_text).split()\n",
    "        doc_score = query_results[query_results['docid'] == docid]['score'].values[0]\n",
    "        for x in range(10):\n",
    "            for term in doc_terms:\n",
    "                if term in customStopwords:\n",
    "                    doc_terms.remove(term)\n",
    "            x += 1\n",
    "\n",
    "        for term in doc_terms:\n",
    "            if term not in terms_scores:\n",
    "                terms_scores[term] = 0\n",
    "            terms_scores[term] += doc_score * calculate_tf_idf(term, doc_text, index)\n",
    "    \n",
    "    sorted_terms = sorted(terms_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "   \n",
    "    \n",
    "    return [term for term, score in sorted_terms[:num_terms]]\n",
    "        \n",
    "\n",
    "\n",
    "# Function to expand query\n",
    "def expand_query(topic):\n",
    "    original_query = topic['query']\n",
    "    #expanded_terms = retrieve_top_docs_terms(original_query)\n",
    "    llm_query = topic['llm_expansion_query']\n",
    "    #print(\"Test\")\n",
    "   \n",
    "    \n",
    "    #print(llm_query)\n",
    "    \n",
    "    llm_terms = retrieve_top_docs_terms(llm_query)\n",
    "    #print(llm_terms)\n",
    "    \n",
    "    expanded_query = ' '.join([original_query] * 6 + llm_terms)\n",
    "\n",
    "    return pt_tokenize(expanded_query)\n",
    "\n",
    "# Wrapper for PyTerrier query expansion\n",
    "pt_expand_query = pt.apply.query(expand_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pipelines with the new query expansion function\n",
    "pipeline_gpt_cot = (gpt_cot >> pt_expand_query) >> bm25\n",
    "pipeline_gpt_sq_fs = (gpt_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_gpt_sq_zs = (gpt_sq_zs >> pt_expand_query) >> bm25\n",
    "\n",
    "pipeline_llama_cot = (llama_cot >> pt_expand_query) >> bm25\n",
    "pipeline_llama_sq_fs = (llama_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_llama_sq_zs = (llama_sq_zs >> pt_expand_query) >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>recall_1000</th>\n",
       "      <th>ndcg_cut_5</th>\n",
       "      <th>ndcg_cut_10</th>\n",
       "      <th>recip_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.834068</td>\n",
       "      <td>0.375888</td>\n",
       "      <td>0.363774</td>\n",
       "      <td>0.572496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25+KL</td>\n",
       "      <td>0.839492</td>\n",
       "      <td>0.379332</td>\n",
       "      <td>0.362127</td>\n",
       "      <td>0.598772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25+GPT-COT</td>\n",
       "      <td>0.847365</td>\n",
       "      <td>0.403628</td>\n",
       "      <td>0.381031</td>\n",
       "      <td>0.604876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BM25+GPT-SQ-FS</td>\n",
       "      <td>0.849050</td>\n",
       "      <td>0.358546</td>\n",
       "      <td>0.346663</td>\n",
       "      <td>0.569714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BM25+GPT-SQ-ZS</td>\n",
       "      <td>0.845811</td>\n",
       "      <td>0.370241</td>\n",
       "      <td>0.352965</td>\n",
       "      <td>0.574072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BM25+Llama-COT</td>\n",
       "      <td>0.839724</td>\n",
       "      <td>0.364432</td>\n",
       "      <td>0.355543</td>\n",
       "      <td>0.568273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BM25+Llama-SQ-FS</td>\n",
       "      <td>0.847496</td>\n",
       "      <td>0.390159</td>\n",
       "      <td>0.362397</td>\n",
       "      <td>0.599283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BM25+Llama-SQ-ZS</td>\n",
       "      <td>0.847622</td>\n",
       "      <td>0.395643</td>\n",
       "      <td>0.372658</td>\n",
       "      <td>0.619827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  recall_1000  ndcg_cut_5  ndcg_cut_10  recip_rank\n",
       "0              BM25     0.834068    0.375888     0.363774    0.572496\n",
       "1           BM25+KL     0.839492    0.379332     0.362127    0.598772\n",
       "2      BM25+GPT-COT     0.847365    0.403628     0.381031    0.604876\n",
       "3    BM25+GPT-SQ-FS     0.849050    0.358546     0.346663    0.569714\n",
       "4    BM25+GPT-SQ-ZS     0.845811    0.370241     0.352965    0.574072\n",
       "5    BM25+Llama-COT     0.839724    0.364432     0.355543    0.568273\n",
       "6  BM25+Llama-SQ-FS     0.847496    0.390159     0.362397    0.599283\n",
       "7  BM25+Llama-SQ-ZS     0.847622    0.395643     0.372658    0.619827"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    retr_systems=[bm25, bm25_kl, pipeline_gpt_cot, pipeline_gpt_sq_fs, pipeline_gpt_sq_zs, pipeline_llama_cot, pipeline_llama_sq_fs, pipeline_llama_sq_zs],\n",
    "    topics=pt_dataset.get_topics(),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25','BM25+KL', 'BM25+GPT-COT', 'BM25+GPT-SQ-FS', 'BM25+GPT-SQ-ZS', 'BM25+Llama-COT', 'BM25+Llama-SQ-FS', 'BM25+Llama-SQ-ZS'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut_10', 'recip_rank']\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab hier gibt es nur noch alte Versuche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bm25_rm3 = bm25 >> pt.rewrite.RM3(index) >> bm25\\nbm25_kl = bm25 >> pt.rewrite.KLQueryExpansion(index) >> bm25'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bm25_rm3 = bm25 >> pt.rewrite.RM3(index) >> bm25\n",
    "bm25_kl = bm25 >> pt.rewrite.KLQueryExpansion(index) >> bm25'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# llm expansions with gpt\\ngpt_cot = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-cot', dataset, prefix='llm_expansion_')\\ngpt_sq_fs = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-sq-fs', dataset, prefix='llm_expansion_')\\ngpt_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-gpt3.5-sq-zs', dataset, prefix='llm_expansion_')\\n\\n# llm expansions with llama\\nllama_cot = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-cot', dataset, prefix='llm_expansion_')\\nllama_sq_fs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-fs', dataset, prefix='llm_expansion_')\\nllama_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-zs', dataset, prefix='llm_expansion_')\\n\\n# llm expansions with flan-ul2\\nflan_cot = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-cot', dataset, prefix='llm_expansion_')\\nflan_sq_fs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-sq-fs', dataset, prefix='llm_expansion_')\\nflan_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-sq-zs', dataset, prefix='llm_expansion_')\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# llm expansions with gpt\n",
    "gpt_cot = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-cot', dataset, prefix='llm_expansion_')\n",
    "gpt_sq_fs = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-sq-fs', dataset, prefix='llm_expansion_')\n",
    "gpt_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-gpt3.5-sq-zs', dataset, prefix='llm_expansion_')\n",
    "\n",
    "# llm expansions with llama\n",
    "llama_cot = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-cot', dataset, prefix='llm_expansion_')\n",
    "llama_sq_fs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-fs', dataset, prefix='llm_expansion_')\n",
    "llama_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-zs', dataset, prefix='llm_expansion_')\n",
    "\n",
    "# llm expansions with flan-ul2\n",
    "flan_cot = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-cot', dataset, prefix='llm_expansion_')\n",
    "flan_sq_fs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-sq-fs', dataset, prefix='llm_expansion_')\n",
    "flan_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-sq-zs', dataset, prefix='llm_expansion_')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokeniser = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\\n\\ndef pt_tokenize(text):\\n    return \\' \\'.join(tokeniser.getTokens(text))\\n\\ndef expand_query(topic):\\n  ret = \\' \\'.join([topic[\\'query\\'], topic[\\'query\\'], topic[\\'query\\'],  topic[\\'query\\'],  topic[\\'query\\'], topic[\\'llm_expansion_query\\']])\\n\\n  # apply the tokenization\\n  return pt_tokenize(ret)\\n\\n# we wrap this into an pyterrier transformer\\n# Documentation: https://pyterrier.readthedocs.io/en/latest/apply.html\\npt_expand_query = pt.apply.query(expand_query)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tokeniser = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "\n",
    "def pt_tokenize(text):\n",
    "    return ' '.join(tokeniser.getTokens(text))\n",
    "\n",
    "def expand_query(topic):\n",
    "  ret = ' '.join([topic['query'], topic['query'], topic['query'],  topic['query'],  topic['query'], topic['llm_expansion_query']])\n",
    "\n",
    "  # apply the tokenization\n",
    "  return pt_tokenize(ret)\n",
    "\n",
    "# we wrap this into an pyterrier transformer\n",
    "# Documentation: https://pyterrier.readthedocs.io/en/latest/apply.html\n",
    "pt_expand_query = pt.apply.query(expand_query)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def pt_tokenize(text):\\n    tokeniser = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\\n    return \\' \\'.join(tokeniser.getTokens(text))\\n\\ndef compute_term_weights(docs, expansion_terms):\\n    term_weights = {term: 0 for term in expansion_terms}\\n    for doc in docs:\\n        for term in expansion_terms:\\n            term_freq = doc[\\'text\\'].count(term)\\n            term_weights[term] += term_freq * doc[\\'score\\']\\n    return term_weights\\n\\ndef expand_query_with_weights(topic, bm25, top_k=10, top_terms=5):\\n    # Erste Abfrage ausführen\\n    initial_results = bm25.transform(pd.DataFrame([topic]))\\n    top_docs = initial_results.head(top_k).to_dict(\\'records\\')\\n    \\n    # Extrahiere die Expansionsterms\\n    expansion_terms = topic[\\'llm_expansion_query\\'].split()\\n    \\n    # Berechne die Termgewichte\\n    term_weights = compute_term_weights(top_docs, expansion_terms)\\n    \\n    # Wähle die Top-Terme basierend auf den Gewichten\\n    sorted_terms = sorted(term_weights.items(), key=lambda item: item[1], reverse=True)\\n    top_expansion_terms = [term for term, weight in sorted_terms[:top_terms]]\\n    \\n    # Erstelle die erweiterte Abfrage\\n    expanded_query = \\' \\'.join([topic[\\'query\\'], topic[\\'query\\'], topic[\\'query\\'],  topic[\\'query\\'],  topic[\\'query\\'], top_expansion_terms])\\n    \\n    # Tokenize the query\\n    return pt_tokenize(expanded_query)\\n\\ndef expand_query_transformer(bm25, top_k=10, top_terms=5):\\n    def _transformer(topics):\\n        topics[\\'query\\'] = topics.apply(lambda row: expand_query_with_weights(row, bm25, top_k, top_terms))\\n        return topics\\n    return pt.apply.query(_transformer)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def pt_tokenize(text):\n",
    "    tokeniser = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "    return ' '.join(tokeniser.getTokens(text))\n",
    "\n",
    "def compute_term_weights(docs, expansion_terms):\n",
    "    term_weights = {term: 0 for term in expansion_terms}\n",
    "    for doc in docs:\n",
    "        for term in expansion_terms:\n",
    "            term_freq = doc['text'].count(term)\n",
    "            term_weights[term] += term_freq * doc['score']\n",
    "    return term_weights\n",
    "\n",
    "def expand_query_with_weights(topic, bm25, top_k=10, top_terms=5):\n",
    "    # Erste Abfrage ausführen\n",
    "    initial_results = bm25.transform(pd.DataFrame([topic]))\n",
    "    top_docs = initial_results.head(top_k).to_dict('records')\n",
    "    \n",
    "    # Extrahiere die Expansionsterms\n",
    "    expansion_terms = topic['llm_expansion_query'].split()\n",
    "    \n",
    "    # Berechne die Termgewichte\n",
    "    term_weights = compute_term_weights(top_docs, expansion_terms)\n",
    "    \n",
    "    # Wähle die Top-Terme basierend auf den Gewichten\n",
    "    sorted_terms = sorted(term_weights.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_expansion_terms = [term for term, weight in sorted_terms[:top_terms]]\n",
    "    \n",
    "    # Erstelle die erweiterte Abfrage\n",
    "    expanded_query = ' '.join([topic['query'], topic['query'], topic['query'],  topic['query'],  topic['query'], top_expansion_terms])\n",
    "    \n",
    "    # Tokenize the query\n",
    "    return pt_tokenize(expanded_query)\n",
    "\n",
    "def expand_query_transformer(bm25, top_k=10, top_terms=5):\n",
    "    def _transformer(topics):\n",
    "        topics['query'] = topics.apply(lambda row: expand_query_with_weights(row, bm25, top_k, top_terms))\n",
    "        return topics\n",
    "    return pt.apply.query(_transformer)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_gpt_cot = (gpt_cot >> expand_query_transformer(bm25)) >> bm25\\npipeline_gpt_sq_fs = (gpt_sq_fs >> expand_query_transformer(bm25)) >> bm25\\npipeline_gpt_sq_zs = (gpt_sq_zs >> expand_query_transformer(bm25)) >> bm25\\n\\npipeline_llama_cot = (llama_cot >> expand_query_transformer(bm25)) >> bm25\\npipeline_llama_sq_fs = (llama_sq_fs >> expand_query_transformer(bm25)) >> bm25\\npipeline_llama_sq_zs = (llama_sq_zs >> expand_query_transformer(bm25)) >> bm25\\n\\npipeline_flan_cot = (flan_cot >> expand_query_transformer(bm25)) >> bm25\\npipeline_flan_sq_fs = (flan_sq_fs >> expand_query_transformer(bm25)) >> bm25\\npipeline_flan_sq_zs = (flan_sq_zs >> expand_query_transformer(bm25)) >> bm25'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pipeline_gpt_cot = (gpt_cot >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_gpt_sq_fs = (gpt_sq_fs >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_gpt_sq_zs = (gpt_sq_zs >> expand_query_transformer(bm25)) >> bm25\n",
    "\n",
    "pipeline_llama_cot = (llama_cot >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_llama_sq_fs = (llama_sq_fs >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_llama_sq_zs = (llama_sq_zs >> expand_query_transformer(bm25)) >> bm25\n",
    "\n",
    "pipeline_flan_cot = (flan_cot >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_flan_sq_fs = (flan_sq_fs >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_flan_sq_zs = (flan_sq_zs >> expand_query_transformer(bm25)) >> bm25'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_gpt_cot = (gpt_cot >> pt_expand_query) >> bm25\\npipeline_gpt_sq_fs = (gpt_sq_fs >> pt_expand_query) >> bm25\\npipeline_gpt_sq_zs = (gpt_sq_zs >> pt_expand_query) >> bm25\\n\\npipeline_llama_cot = (llama_cot >> pt_expand_query) >> bm25\\npipeline_llama_sq_fs = (llama_sq_fs >> pt_expand_query) >> bm25\\npipeline_llama_sq_zs = (llama_sq_zs >> pt_expand_query) >> bm25\\n\\npipeline_flan_cot = (flan_cot >> pt_expand_query) >> bm25\\npipeline_flan_sq_fs = (flan_sq_fs >> pt_expand_query) >> bm25\\npipeline_flan_sq_zs = (flan_sq_zs >> pt_expand_query) >> bm25'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pipeline_gpt_cot = (gpt_cot >> pt_expand_query) >> bm25\n",
    "pipeline_gpt_sq_fs = (gpt_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_gpt_sq_zs = (gpt_sq_zs >> pt_expand_query) >> bm25\n",
    "\n",
    "pipeline_llama_cot = (llama_cot >> pt_expand_query) >> bm25\n",
    "pipeline_llama_sq_fs = (llama_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_llama_sq_zs = (llama_sq_zs >> pt_expand_query) >> bm25\n",
    "\n",
    "pipeline_flan_cot = (flan_cot >> pt_expand_query) >> bm25\n",
    "pipeline_flan_sq_fs = (flan_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_flan_sq_zs = (flan_sq_zs >> pt_expand_query) >> bm25'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bo1 Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bo1_expansion = bm25 >> pt.rewrite.Bo1QueryExpansion(index)\\n# build final pipeline for retrieval\\nbm25_bo1 = bo1_expansion >> bm25'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bo1_expansion = bm25 >> pt.rewrite.Bo1QueryExpansion(index)\n",
    "# build final pipeline for retrieval\n",
    "bm25_bo1 = bo1_expansion >> bm25'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pt.Experiment(\\n    retr_systems=[bm25, bm25_stopwords, bm25_bo1, bm25_rm3, bm25_kl, pipeline_gpt_cot, pipeline_gpt_sq_fs, pipeline_gpt_sq_zs, pipeline_llama_cot, pipeline_llama_sq_fs, pipeline_llama_sq_zs, pipeline_flan_cot, pipeline_flan_sq_fs, pipeline_flan_sq_zs],\\n    topics=pt_dataset.get_topics('text'),\\n    qrels=pt_dataset.get_qrels(),\\n    names=['BM25', 'BM25_stopwords','BM25_Bo1', 'BM25+RM3', 'BM25+KL', 'BM25+GPT-COT', 'BM25+GPT-SQ-FS', 'BM25+GPT-SQ-ZS', 'BM25+Llama-COT', 'BM25+Llama-SQ-FS', 'BM25+Llama-SQ-ZS', 'BM25+Flan-COT', 'BM25+Flan-SQ-FS', 'BM25+Flan-SQ-ZS'],\\n    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\\n)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    retr_systems=[bm25, bm25_stopwords, bm25_bo1, bm25_rm3, bm25_kl, pipeline_gpt_cot, pipeline_gpt_sq_fs, pipeline_gpt_sq_zs, pipeline_llama_cot, pipeline_llama_sq_fs, pipeline_llama_sq_zs, pipeline_flan_cot, pipeline_flan_sq_fs, pipeline_flan_sq_zs],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25', 'BM25_stopwords','BM25_Bo1', 'BM25+RM3', 'BM25+KL', 'BM25+GPT-COT', 'BM25+GPT-SQ-FS', 'BM25+GPT-SQ-ZS', 'BM25+Llama-COT', 'BM25+Llama-SQ-FS', 'BM25+Llama-SQ-ZS', 'BM25+Flan-COT', 'BM25+Flan-SQ-FS', 'BM25+Flan-SQ-ZS'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pt.Experiment(\\n    retr_systems=[bm25, bm25_rm3, bm25_kl, pipeline_gpt_cot, pipeline_gpt_sq_fs, pipeline_gpt_sq_zs, pipeline_llama_cot, pipeline_llama_sq_fs, pipeline_llama_sq_zs, pipeline_flan_cot, pipeline_flan_sq_fs, pipeline_flan_sq_zs],\\n    topics=pt_dataset.get_topics('text'),\\n    qrels=pt_dataset.get_qrels(),\\n    names=['BM25', 'BM25+RM3', 'BM25+KL', 'BM25+GPT-COT', 'BM25+GPT-SQ-FS', 'BM25+GPT-SQ-ZS', 'BM25+Llama-COT', 'BM25+Llama-SQ-FS', 'BM25+Llama-SQ-ZS', 'BM25+Flan-COT', 'BM25+Flan-SQ-FS', 'BM25+Flan-SQ-ZS'],\\n    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\\n)\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    retr_systems=[bm25, bm25_rm3, bm25_kl, pipeline_gpt_cot, pipeline_gpt_sq_fs, pipeline_gpt_sq_zs, pipeline_llama_cot, pipeline_llama_sq_fs, pipeline_llama_sq_zs, pipeline_flan_cot, pipeline_flan_sq_fs, pipeline_flan_sq_zs],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25', 'BM25+RM3', 'BM25+KL', 'BM25+GPT-COT', 'BM25+GPT-SQ-FS', 'BM25+GPT-SQ-ZS', 'BM25+Llama-COT', 'BM25+Llama-SQ-FS', 'BM25+Llama-SQ-ZS', 'BM25+Flan-COT', 'BM25+Flan-SQ-FS', 'BM25+Flan-SQ-ZS'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"p1 =   pipeline_gpt_sq_zs >> bm25_stopwords\\np2 = pipeline_llama_sq_fs >> bm25_stopwords\\np3 = bm25_bo1 >> bm25_stopwords\\np4 = p1 >> bm25\\np5 = p2 >> bm25\\np6 = p3 >> bm25\\n\\np7 = bm25_stopwords >> pipeline_gpt_cot\\np8 = p7 >> bm25_stopwords\\np9 = p4 >> bm25_stopwords\\n\\npt.Experiment(\\n    retr_systems=[p1, p2, p3, p4, p5, p6, p7, p8, p9],\\n    topics=pt_dataset.get_topics('text'),\\n    qrels=pt_dataset.get_qrels(),\\n    names=['Prototype1', 'Prototype2', 'Prototype3', 'Prototype4', 'Prototype5', 'Prototype6', 'Prototype7', 'Prototype8', 'Prototype9'],\\n    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\\n)\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''p1 =   pipeline_gpt_sq_zs >> bm25_stopwords\n",
    "p2 = pipeline_llama_sq_fs >> bm25_stopwords\n",
    "p3 = bm25_bo1 >> bm25_stopwords\n",
    "p4 = p1 >> bm25\n",
    "p5 = p2 >> bm25\n",
    "p6 = p3 >> bm25\n",
    "\n",
    "p7 = bm25_stopwords >> pipeline_gpt_cot\n",
    "p8 = p7 >> bm25_stopwords\n",
    "p9 = p4 >> bm25_stopwords\n",
    "\n",
    "pt.Experiment(\n",
    "    retr_systems=[p1, p2, p3, p4, p5, p6, p7, p8, p9],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['Prototype1', 'Prototype2', 'Prototype3', 'Prototype4', 'Prototype5', 'Prototype6', 'Prototype7', 'Prototype8', 'Prototype9'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\n",
    ")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
