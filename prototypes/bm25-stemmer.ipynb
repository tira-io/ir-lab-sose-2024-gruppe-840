{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "ensure_pyterrier_is_loaded()\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from tqdm import tqdm\n",
    "from jnius import autoclass\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "tira = Client()\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset and the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "dataset = 'ir-acl-anthology-20240504-training'\n",
    "pt_dataset = pt.get_dataset(f'irds:ir-lab-sose-2024/{dataset}')\n",
    "\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmer and Lemmatizer  (funktioniert leider nicht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variante 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def stem(t):\\n    lemmatizer = WordNetLemmatizer()\\n    return lemmatizer.lemmatize(t)\\n\\nlemmatizer = WordNetLemmatizer()\\n\\n\\nindexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=lemmatizer)\\nindex_ref = indexer.index(pt_dataset.get_corpus_iter())\\nindex = pt.IndexFactory.of(index_ref)\\n\\nbm25_stem = pt.BatchRetrieve(index, wmodel=\"BM25\")\\n\\n\\n\\ndef stem(t):\\n    stemmer = LancasterStemmer()\\n    return stemmer.stem(t)\\n\\nstemmer = LancasterStemmer()\\n\\nindexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=stemmer)\\nindex_ref = indexer.index(pt_dataset.get_corpus_iter())\\nindex = pt.IndexFactory.of(index_ref)\\n\\n\\nbm25_lem = pt.BatchRetrieve(index, wmodel=\"BM25\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def stem(t):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(t)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=lemmatizer)\n",
    "index_ref = indexer.index(pt_dataset.get_corpus_iter())\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "bm25_stem = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "\n",
    "\n",
    "def stem(t):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return stemmer.stem(t)\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, stemmer=stemmer)\n",
    "index_ref = indexer.index(pt_dataset.get_corpus_iter())\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "\n",
    "bm25_lem = pt.BatchRetrieve(index, wmodel=\"BM25\")'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variante 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def stem_text(text):\\n    stemmer = LancasterStemmer()\\n    return \" \".join([stemmer.stem(word) for word in nltk.word_tokenize(text)])\\n\\ndef lemmatize_text(text):\\n    lemmatizer = WordNetLemmatizer()\\n    return \" \".join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\\n\\ndef document_iterator(docs, transform_func):\\n    for doc in docs:\\n        doc[\"text\"] = transform_func(doc[\"text\"])\\n        yield doc\\n\\n\\ntransform_func = stem_text  # oder lemmatize_text\\n\\n\\nindexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True)\\nindex_ref = indexer.index(document_iterator(pt_dataset.get_corpus_iter(), transform_func))\\n\\nclass NLTKStemmerTransformer(pt.Transformer):\\n    def __init__(self, stemmer):\\n        self.stemmer = stemmer\\n\\n    def transform(self, queries):\\n        queries[\\'query\\'] = queries[\\'query\\'].apply(stem_text)\\n        return queries\\n\\n\\nstemmer = LancasterStemmer()\\nnltk_stemmer_transformer = NLTKStemmerTransformer(stemmer)\\n\\n\\nindex = pt.IndexFactory.of(index_ref)\\n\\n\\nbm25_stem = nltk_stemmer_transformer >> pt.BatchRetrieve(index, wmodel=\"BM25\")'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def stem_text(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in nltk.word_tokenize(text)])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\n",
    "\n",
    "def document_iterator(docs, transform_func):\n",
    "    for doc in docs:\n",
    "        doc[\"text\"] = transform_func(doc[\"text\"])\n",
    "        yield doc\n",
    "\n",
    "\n",
    "transform_func = stem_text  # oder lemmatize_text\n",
    "\n",
    "\n",
    "indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True)\n",
    "index_ref = indexer.index(document_iterator(pt_dataset.get_corpus_iter(), transform_func))\n",
    "\n",
    "class NLTKStemmerTransformer(pt.Transformer):\n",
    "    def __init__(self, stemmer):\n",
    "        self.stemmer = stemmer\n",
    "\n",
    "    def transform(self, queries):\n",
    "        queries['query'] = queries['query'].apply(stem_text)\n",
    "        return queries\n",
    "\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "nltk_stemmer_transformer = NLTKStemmerTransformer(stemmer)\n",
    "\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "\n",
    "bm25_stem = nltk_stemmer_transformer >> pt.BatchRetrieve(index, wmodel=\"BM25\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npt.Experiment(\\n    retr_systems=[bm25, bm25_stem],\\n    topics=pt_dataset.get_topics('text'),\\n    qrels=pt_dataset.get_qrels(),\\n    names=['BM25', 'bm25_stem'],\\n    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\\n)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pt.Experiment(\n",
    "    retr_systems=[bm25, bm25_stem],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25', 'bm25_stem'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\n",
    ")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider kommt immer wieder 0.0 raus egal welche Variante ich benutze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Expansion by Query Prediction with docT5query\n",
    "The basic idea is to train a model, that when given an input document, generates questions that the document might answer (or more broadly, queries for which the document might be relevant). These predicted questions (or queries) are then appended to the original documents, which are then indexed as before. The docT5query model gets its name from the use of T5 as the expansion model.\n",
    "\n",
    "The primary advantage of this approach is that expensive neural inference is pushed to indexing time, which means that \"bag of words\" queries against an inverted index built on the augmented document collection are only slightly slower (due to longer documents) — but the retrieval results are much better.\n",
    "\n",
    "First we check, if our corpus has a high recall or a lower. Our Corpus in this case is the union of the IR Anthology and the ACL Anthology. The recall may change if we use another corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\\n\\npt.Experiment(\\n    retr_systems=[bm25],\\n    topics=pt_dataset.get_topics(\\'text\\'),\\n    qrels=pt_dataset.get_qrels(),\\n    names=[\\'BM25\\'],\\n    eval_metrics=[\\'recall_1000\\']\\n)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "pt.Experiment(\n",
    "    retr_systems=[bm25],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25'],\n",
    "    eval_metrics=['recall_1000']\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have already a high recall. This is important for the way we implement the docT5query.\n",
    "More information about the implementation in the [Tutorial](https://github.com/tira-io/teaching-ir-with-shared-tasks/blob/main/tutorials/tutorial-doc-t5-query.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def doc_t5_query(dataset):\\n    docs = tira.get_run_output(\\'ir-benchmarks/seanmacavaney/DocT5Query\\', dataset) + \\'/documents.jsonl.gz\\'\\n    with gzip.open(docs, \\'rt\\') as f:\\n        for l in tqdm(f):\\n            l = json.loads(l)\\n            l[\\'text\\'] = l[\\'querygen\\']\\n            l[\\'docno\\'] = l[\\'doc_id\\']\\n            del l[\\'doc_id\\']\\n            del l[\\'querygen\\']\\n            yield l\\n\\ndef doc_t5_query_index(dataset):\\n    indexer = pt.IterDictIndexer(\"/tmp/index2\", overwrite=True, meta={\\'docno\\': 100, \\'text\\': 20480})\\n    index_ref = indexer.index(doc_t5_query(dataset))\\n    return pt.IndexFactory.of(index_ref)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def doc_t5_query(dataset):\n",
    "    docs = tira.get_run_output('ir-benchmarks/seanmacavaney/DocT5Query', dataset) + '/documents.jsonl.gz'\n",
    "    with gzip.open(docs, 'rt') as f:\n",
    "        for l in tqdm(f):\n",
    "            l = json.loads(l)\n",
    "            l['text'] = l['querygen']\n",
    "            l['docno'] = l['doc_id']\n",
    "            del l['doc_id']\n",
    "            del l['querygen']\n",
    "            yield l\n",
    "\n",
    "def doc_t5_query_index(dataset):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index2\", overwrite=True, meta={'docno': 100, 'text': 20480})\n",
    "    index_ref = indexer.index(doc_t5_query(dataset))\n",
    "    return pt.IndexFactory.of(index_ref)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexD = doc_t5_query_index(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"docs_retrieved_by_bm25 = {}\\n\\nbm25_result = bm25(pt_dataset.get_topics('title'))\\n\\nfor _, i in tqdm(bm25_result.iterrows()):\\n    qid, docno = str(i['qid']), str(i['docno'])\\n\\n    if qid not in docs_retrieved_by_bm25:\\n        docs_retrieved_by_bm25[qid] = set()\\n    \\n    docs_retrieved_by_bm25[qid].add(docno)\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''docs_retrieved_by_bm25 = {}\n",
    "\n",
    "bm25_result = bm25(pt_dataset.get_topics('title'))\n",
    "\n",
    "for _, i in tqdm(bm25_result.iterrows()):\n",
    "    qid, docno = str(i['qid']), str(i['docno'])\n",
    "\n",
    "    if qid not in docs_retrieved_by_bm25:\n",
    "        docs_retrieved_by_bm25[qid] = set()\n",
    "    \n",
    "    docs_retrieved_by_bm25[qid].add(docno)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"omit_already_retrieved_docs = lambda i: i[i.apply(lambda j: str(j['docno']) not in docs_retrieved_by_bm25[str(j['qid'])], axis=1)]\\nomit_already_retrieved_docs = pt.apply.generic(omit_already_retrieved_docs)\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''omit_already_retrieved_docs = lambda i: i[i.apply(lambda j: str(j['docno']) not in docs_retrieved_by_bm25[str(j['qid'])], axis=1)]\n",
    "omit_already_retrieved_docs = pt.apply.generic(omit_already_retrieved_docs)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bm25_doct5query = pt.BatchRetrieve(indexD, wmodel=\"BM25\")\\nbm25_doct5query_new = bm25_doct5query >> omit_already_retrieved_docs'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bm25_doct5query = pt.BatchRetrieve(indexD, wmodel=\"BM25\")\n",
    "bm25_doct5query_new = bm25_doct5query >> omit_already_retrieved_docs'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DocT5Query bringt uns im normalen dataset nicht so viel, also lassen wir es weg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:  71%|███████   | 90188/126958 [00:17<00:05, 6576.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:10:33.493 [ForkJoinPool-2-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (2020.mir_conference-2020.1) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:22<00:00, 5561.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:10:41.485 [ForkJoinPool-2-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 3 empty documents\n"
     ]
    }
   ],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "def create_index(documents, stopwords):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=customStopwords)\n",
    "    index_ref = indexer.index(documents)\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "\n",
    "customStopwords =[\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', \n",
    "    'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "    'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', \n",
    "    'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \n",
    "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
    "    'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', \n",
    "    'will', 'just', 'don', 'should', 'now', 'e', 'n', 'd', 'o'\n",
    "]\n",
    "\n",
    "index = create_index(pt_dataset.get_corpus_iter(), customStopwords)\n",
    "\n",
    "bm25_stopwords = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion with Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure BM25 is initialized\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "bm25_rm3 = bm25 >> pt.rewrite.RM3(index) >> bm25\n",
    "bm25_kl = bm25 >> pt.rewrite.KLQueryExpansion(index) >> bm25\n",
    "\n",
    "# llm expansions with gpt\n",
    "gpt_cot = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-cot', dataset, prefix='llm_expansion_')\n",
    "gpt_sq_fs = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-sq-fs', dataset, prefix='llm_expansion_')\n",
    "gpt_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-gpt3.5-sq-zs', dataset, prefix='llm_expansion_')\n",
    "\n",
    "# llm expansions with llama\n",
    "llama_cot = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-cot', dataset, prefix='llm_expansion_')\n",
    "llama_sq_fs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-fs', dataset, prefix='llm_expansion_')\n",
    "llama_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-zs', dataset, prefix='llm_expansion_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 52923.97it/s]\n"
     ]
    }
   ],
   "source": [
    "tokeniser = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "\n",
    "documents = []\n",
    "for doc in pt_dataset.get_corpus_iter():\n",
    "        documents.append({\n",
    "        'docno': doc['docno'],\n",
    "        'text': doc['text'],\n",
    "})   \n",
    "\n",
    "def pt_tokenize(text):\n",
    "    return ' '.join(tokeniser.getTokens(text))\n",
    "\n",
    "# we wrap this into an pyterrier transformer\n",
    "# Documentation: https://pyterrier.readthedocs.io/en/latest/apply.html\n",
    "\n",
    "def calculate_tf_idf(term, doc_text, index):\n",
    "    # Implement your TF-IDF calculation logic here\n",
    "    # Example placeholder implementation:\n",
    "    tf = doc_text.lower().count(term.lower()) / len(doc_text.split())\n",
    "    #print(tf)\n",
    "    num_docs = index.getCollectionStatistics().getNumberOfDocuments()\n",
    "    lexicon = index.getLexicon()\n",
    "    entry = lexicon.getLexiconEntry(term.lower())\n",
    "    doc_freq = entry.getDocumentFrequency() if entry is not None else 0\n",
    "    idf = math.log((num_docs + 1) / (doc_freq + 1)) + 1\n",
    "\n",
    "    #print(tf * idf)\n",
    "    return tf * idf\n",
    "\n",
    "# Define a function to retrieve top documents and extract terms with scores\n",
    "def retrieve_top_docs_terms(query, num_docs=10, num_terms=10):\n",
    "    # Annahme: bm25 ist ein BatchRetrieve-Objekt\n",
    "    query_results = bm25.search(query)\n",
    "    top_docs = query_results.head(num_docs)['docid'].tolist()\n",
    "    terms_scores = {}\n",
    "    \n",
    "    for docid in top_docs:\n",
    "        \n",
    "        doc_text = documents[docid]['text']\n",
    "        doc_terms = pt_tokenize(doc_text).split()\n",
    "        doc_score = query_results[query_results['docid'] == docid]['score'].values[0]\n",
    "        for x in range(10):\n",
    "            for term in doc_terms:\n",
    "                if term in customStopwords:\n",
    "                    doc_terms.remove(term)\n",
    "            x += 1\n",
    "\n",
    "        for term in doc_terms:\n",
    "            if term not in terms_scores:\n",
    "                terms_scores[term] = 0\n",
    "            terms_scores[term] += doc_score * calculate_tf_idf(term, doc_text, index)\n",
    "    \n",
    "    sorted_terms = sorted(terms_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [term for term, score in sorted_terms[:num_terms]]\n",
    "        \n",
    "\n",
    "\n",
    "# Function to expand query\n",
    "def expand_query(topic):\n",
    "    original_query = topic['query']\n",
    "    #expanded_terms = retrieve_top_docs_terms(original_query)\n",
    "    llm_query = topic['llm_expansion_query']\n",
    "    #print(\"Test\")\n",
    "   \n",
    "    \n",
    "    #print(llm_query)\n",
    "    \n",
    "    llm_terms = retrieve_top_docs_terms(llm_query)\n",
    "    #print(llm_terms)\n",
    "    \n",
    "    expanded_query = ' '.join([original_query] * 6 + llm_terms)\n",
    "\n",
    "    return pt_tokenize(expanded_query)\n",
    "\n",
    "# Wrapper for PyTerrier query expansion\n",
    "pt_expand_query = pt.apply.query(expand_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pipelines with the new query expansion function\n",
    "pipeline_gpt_cot = (gpt_cot >> pt_expand_query) >> bm25\n",
    "pipeline_gpt_sq_fs = (gpt_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_gpt_sq_zs = (gpt_sq_zs >> pt_expand_query) >> bm25\n",
    "\n",
    "pipeline_llama_cot = (llama_cot >> pt_expand_query) >> bm25\n",
    "pipeline_llama_sq_fs = (llama_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_llama_sq_zs = (llama_sq_zs >> pt_expand_query) >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: Failed to process qid 1 'Improving the effectiveness of a retrieval system involves enhancing its ability to accurately and efficiently retrieve relevant information in response to user queries. This can be achieved through various strategies such as:\n\n1. **Improved indexing**: By ensuring that all relevant content is properly indexed, including metadata and keywords, the retrieval system can more easily locate and retrieve relevant information.\n\n2. **Enhanced search algorithms**: Implementing advanced search algorithms can help the system better understand user queries and match them with relevant content, improving the accuracy of search results.\n\n3. **User feedback**: Incorporating user feedback mechanisms can help the system learn from user interactions and improve its performance over time by adjusting search results based on user preferences.\n\n4. **Personalization**: Tailoring search results to individual user preferences and behavior can improve the relevance of retrieved information and enhance the overall user experience.\n\n5. **Integration of AI and machine learning**: Utilizing artificial intelligence and machine learning technologies can help the retrieval system continuously learn and adapt to changing user needs and preferences' -- Lexical error at line 3, column 4.  Encountered: \"*\" (42), after : \"\" org.terrier.querying.parser.QueryParserException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJavaException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretr_systems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mbm25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25_kl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_gpt_cot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_gpt_sq_fs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_gpt_sq_zs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_llama_cot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_llama_sq_fs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_llama_sq_zs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_qrels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+KL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+GPT-COT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+GPT-SQ-FS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+GPT-SQ-ZS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Llama-COT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Llama-SQ-FS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Llama-SQ-ZS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecall_1000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mndcg_cut_5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mndcg_cut_10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecip_rank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/pipelines.py:471\u001b[0m, in \u001b[0;36mExperiment\u001b[0;34m(retr_systems, topics, qrels, eval_metrics, names, perquery, dataframe, batch_size, filter_by_qrels, filter_by_topics, baseline, test, correction, correction_alpha, highlight, round, verbose, save_dir, save_mode, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     save_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.res.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[0;32m--> 471\u001b[0m time, evalMeasuresDict \u001b[38;5;241m=\u001b[39m \u001b[43m_run_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackfill_qids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_topic_qids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m baseline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     evalDictsPerQ\u001b[38;5;241m.\u001b[39mappend(evalMeasuresDict)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/pipelines.py:192\u001b[0m, in \u001b[0;36m_run_and_evaluate\u001b[0;34m(system, topics, qrels, metrics, pbar, save_mode, save_file, perquery, batch_size, backfill_qids)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m#transformer, evaluate all queries at once\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     starttime \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m--> 192\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m    194\u001b[0m     runtime \u001b[38;5;241m=\u001b[39m  (endtime \u001b[38;5;241m-\u001b[39m starttime) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000.\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:335\u001b[0m, in \u001b[0;36mComposedPipeline.transform\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, topics):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 335\u001b[0m         topics \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:335\u001b[0m, in \u001b[0;36mComposedPipeline.transform\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, topics):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 335\u001b[0m         topics \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/apply_base.py:217\u001b[0m, in \u001b[0;36mApplyQueryTransformer.transform\u001b[0;34m(self, inputRes)\u001b[0m\n\u001b[1;32m    215\u001b[0m     outputRes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputRes\u001b[38;5;241m.\u001b[39mprogress_apply(fn, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     outputRes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43moutputRes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputRes\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 963\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 979\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    981\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    982\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    983\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[25], line 69\u001b[0m, in \u001b[0;36mexpand_query\u001b[0;34m(topic)\u001b[0m\n\u001b[1;32m     63\u001b[0m llm_query \u001b[38;5;241m=\u001b[39m topic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_expansion_query\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#print(\"Test\")\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#print(llm_query)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m llm_terms \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_top_docs_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#print(llm_terms)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m expanded_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([original_query] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m+\u001b[39m llm_terms)\n",
      "Cell \u001b[0;32mIn[25], line 33\u001b[0m, in \u001b[0;36mretrieve_top_docs_terms\u001b[0;34m(query, num_docs, num_terms)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_top_docs_terms\u001b[39m(query, num_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_terms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Annahme: bm25 ist ein BatchRetrieve-Objekt\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     query_results \u001b[38;5;241m=\u001b[39m \u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     top_docs \u001b[38;5;241m=\u001b[39m query_results\u001b[38;5;241m.\u001b[39mhead(num_docs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     35\u001b[0m     terms_scores \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:169\u001b[0m, in \u001b[0;36mTransformer.search\u001b[0;34m(self, query, qid, sort)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m    168\u001b[0m queryDf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([[qid, query]], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 169\u001b[0m rtr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueryDf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rtr\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rtr\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    171\u001b[0m     rtr \u001b[38;5;241m=\u001b[39m rtr\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;28;01mTrue\u001b[39;00m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/batchretrieve.py:441\u001b[0m, in \u001b[0;36mBatchRetrieve.transform\u001b[0;34m(self, queries)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28miter\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m), total\u001b[38;5;241m=\u001b[39mqueries\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocno_provided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocno_provided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocid_provided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocid_provided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_provided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores_provided\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m         results\u001b[38;5;241m.\u001b[39mextend(res)\n\u001b[1;32m    444\u001b[0m res_dt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocid\u001b[39m\u001b[38;5;124m'\u001b[39m ] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/batchretrieve.py:352\u001b[0m, in \u001b[0;36mBatchRetrieve._retrieve_one\u001b[0;34m(self, row, input_results, docno_provided, docid_provided, scores_provided)\u001b[0m\n\u001b[1;32m    349\u001b[0m     srq\u001b[38;5;241m.\u001b[39msetControl(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatching\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.terrier.matching.ScoringMatching\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m srq\u001b[38;5;241m.\u001b[39mgetControl(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatching\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# now ask Terrier to run the request\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunSearchRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m result \u001b[38;5;241m=\u001b[39m srq\u001b[38;5;241m.\u001b[39mgetResults()\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# check we got all of the expected metadata (if the resultset has a size at all)\u001b[39;00m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:877\u001b[0m, in \u001b[0;36mjnius.JavaMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:971\u001b[0m, in \u001b[0;36mjnius.JavaMethod.call_method\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_utils.pxi:79\u001b[0m, in \u001b[0;36mjnius.check_exception\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mJavaException\u001b[0m: JVM exception occurred: Failed to process qid 1 'Improving the effectiveness of a retrieval system involves enhancing its ability to accurately and efficiently retrieve relevant information in response to user queries. This can be achieved through various strategies such as:\n\n1. **Improved indexing**: By ensuring that all relevant content is properly indexed, including metadata and keywords, the retrieval system can more easily locate and retrieve relevant information.\n\n2. **Enhanced search algorithms**: Implementing advanced search algorithms can help the system better understand user queries and match them with relevant content, improving the accuracy of search results.\n\n3. **User feedback**: Incorporating user feedback mechanisms can help the system learn from user interactions and improve its performance over time by adjusting search results based on user preferences.\n\n4. **Personalization**: Tailoring search results to individual user preferences and behavior can improve the relevance of retrieved information and enhance the overall user experience.\n\n5. **Integration of AI and machine learning**: Utilizing artificial intelligence and machine learning technologies can help the retrieval system continuously learn and adapt to changing user needs and preferences' -- Lexical error at line 3, column 4.  Encountered: \"*\" (42), after : \"\" org.terrier.querying.parser.QueryParserException"
     ]
    }
   ],
   "source": [
    "experiment = pt.Experiment(\n",
    "    retr_systems=[bm25, bm25_kl, pipeline_gpt_cot, pipeline_gpt_sq_fs, pipeline_gpt_sq_zs, pipeline_llama_cot, pipeline_llama_sq_fs, pipeline_llama_sq_zs],\n",
    "    topics=pt_dataset.get_topics(),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25','BM25+KL', 'BM25+GPT-COT', 'BM25+GPT-SQ-FS', 'BM25+GPT-SQ-ZS', 'BM25+Llama-COT', 'BM25+Llama-SQ-FS', 'BM25+Llama-SQ-ZS'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut_10', 'recip_rank']\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab hier gibt es nur noch alte Versuche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''bm25_rm3 = bm25 >> pt.rewrite.RM3(index) >> bm25\n",
    "bm25_kl = bm25 >> pt.rewrite.KLQueryExpansion(index) >> bm25'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# llm expansions with gpt\n",
    "gpt_cot = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-cot', dataset, prefix='llm_expansion_')\n",
    "gpt_sq_fs = tira.pt.transform_queries('workshop-on-open-web-search/tu-dresden-03/qe-gpt3.5-sq-fs', dataset, prefix='llm_expansion_')\n",
    "gpt_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-gpt3.5-sq-zs', dataset, prefix='llm_expansion_')\n",
    "\n",
    "# llm expansions with llama\n",
    "llama_cot = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-cot', dataset, prefix='llm_expansion_')\n",
    "llama_sq_fs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-fs', dataset, prefix='llm_expansion_')\n",
    "llama_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-llama-sq-zs', dataset, prefix='llm_expansion_')\n",
    "\n",
    "# llm expansions with flan-ul2\n",
    "flan_cot = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-cot', dataset, prefix='llm_expansion_')\n",
    "flan_sq_fs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-sq-fs', dataset, prefix='llm_expansion_')\n",
    "flan_sq_zs = tira.pt.transform_queries('ir-benchmarks/tu-dresden-03/qe-flan-ul2-sq-zs', dataset, prefix='llm_expansion_')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tokeniser = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "\n",
    "def pt_tokenize(text):\n",
    "    return ' '.join(tokeniser.getTokens(text))\n",
    "\n",
    "def expand_query(topic):\n",
    "  ret = ' '.join([topic['query'], topic['query'], topic['query'],  topic['query'],  topic['query'], topic['llm_expansion_query']])\n",
    "\n",
    "  # apply the tokenization\n",
    "  return pt_tokenize(ret)\n",
    "\n",
    "# we wrap this into an pyterrier transformer\n",
    "# Documentation: https://pyterrier.readthedocs.io/en/latest/apply.html\n",
    "pt_expand_query = pt.apply.query(expand_query)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def pt_tokenize(text):\n",
    "    tokeniser = pt.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "    return ' '.join(tokeniser.getTokens(text))\n",
    "\n",
    "def compute_term_weights(docs, expansion_terms):\n",
    "    term_weights = {term: 0 for term in expansion_terms}\n",
    "    for doc in docs:\n",
    "        for term in expansion_terms:\n",
    "            term_freq = doc['text'].count(term)\n",
    "            term_weights[term] += term_freq * doc['score']\n",
    "    return term_weights\n",
    "\n",
    "def expand_query_with_weights(topic, bm25, top_k=10, top_terms=5):\n",
    "    # Erste Abfrage ausführen\n",
    "    initial_results = bm25.transform(pd.DataFrame([topic]))\n",
    "    top_docs = initial_results.head(top_k).to_dict('records')\n",
    "    \n",
    "    # Extrahiere die Expansionsterms\n",
    "    expansion_terms = topic['llm_expansion_query'].split()\n",
    "    \n",
    "    # Berechne die Termgewichte\n",
    "    term_weights = compute_term_weights(top_docs, expansion_terms)\n",
    "    \n",
    "    # Wähle die Top-Terme basierend auf den Gewichten\n",
    "    sorted_terms = sorted(term_weights.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_expansion_terms = [term for term, weight in sorted_terms[:top_terms]]\n",
    "    \n",
    "    # Erstelle die erweiterte Abfrage\n",
    "    expanded_query = ' '.join([topic['query'], topic['query'], topic['query'],  topic['query'],  topic['query'], top_expansion_terms])\n",
    "    \n",
    "    # Tokenize the query\n",
    "    return pt_tokenize(expanded_query)\n",
    "\n",
    "def expand_query_transformer(bm25, top_k=10, top_terms=5):\n",
    "    def _transformer(topics):\n",
    "        topics['query'] = topics.apply(lambda row: expand_query_with_weights(row, bm25, top_k, top_terms))\n",
    "        return topics\n",
    "    return pt.apply.query(_transformer)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pipeline_gpt_cot = (gpt_cot >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_gpt_sq_fs = (gpt_sq_fs >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_gpt_sq_zs = (gpt_sq_zs >> expand_query_transformer(bm25)) >> bm25\n",
    "\n",
    "pipeline_llama_cot = (llama_cot >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_llama_sq_fs = (llama_sq_fs >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_llama_sq_zs = (llama_sq_zs >> expand_query_transformer(bm25)) >> bm25\n",
    "\n",
    "pipeline_flan_cot = (flan_cot >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_flan_sq_fs = (flan_sq_fs >> expand_query_transformer(bm25)) >> bm25\n",
    "pipeline_flan_sq_zs = (flan_sq_zs >> expand_query_transformer(bm25)) >> bm25'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pipeline_gpt_cot = (gpt_cot >> pt_expand_query) >> bm25\n",
    "pipeline_gpt_sq_fs = (gpt_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_gpt_sq_zs = (gpt_sq_zs >> pt_expand_query) >> bm25\n",
    "\n",
    "pipeline_llama_cot = (llama_cot >> pt_expand_query) >> bm25\n",
    "pipeline_llama_sq_fs = (llama_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_llama_sq_zs = (llama_sq_zs >> pt_expand_query) >> bm25\n",
    "\n",
    "pipeline_flan_cot = (flan_cot >> pt_expand_query) >> bm25\n",
    "pipeline_flan_sq_fs = (flan_sq_fs >> pt_expand_query) >> bm25\n",
    "pipeline_flan_sq_zs = (flan_sq_zs >> pt_expand_query) >> bm25'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bo1 Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''bo1_expansion = bm25 >> pt.rewrite.Bo1QueryExpansion(index)\n",
    "# build final pipeline for retrieval\n",
    "bm25_bo1 = bo1_expansion >> bm25'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:16:21.735 [main] WARN org.terrier.querying.RM1 - Did not identify any usable candidate expansion terms from docid 125137 among 6 possibilities\n",
      "18:16:22.054 [main] WARN org.terrier.querying.RM1 - Did not identify any usable candidate expansion terms from docid 116910 among 5 possibilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>recall_1000</th>\n",
       "      <th>ndcg_cut_5</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>recip_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.825376</td>\n",
       "      <td>0.393650</td>\n",
       "      <td>0.374041</td>\n",
       "      <td>0.579877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25_stopwords</td>\n",
       "      <td>0.834410</td>\n",
       "      <td>0.382805</td>\n",
       "      <td>0.367656</td>\n",
       "      <td>0.581239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25_Bo1</td>\n",
       "      <td>0.819550</td>\n",
       "      <td>0.352757</td>\n",
       "      <td>0.344307</td>\n",
       "      <td>0.540783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BM25+RM3</td>\n",
       "      <td>0.807109</td>\n",
       "      <td>0.339887</td>\n",
       "      <td>0.320692</td>\n",
       "      <td>0.544242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BM25+KL</td>\n",
       "      <td>0.821885</td>\n",
       "      <td>0.351864</td>\n",
       "      <td>0.347616</td>\n",
       "      <td>0.549747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BM25+GPT-COT</td>\n",
       "      <td>0.825698</td>\n",
       "      <td>0.320591</td>\n",
       "      <td>0.305605</td>\n",
       "      <td>0.524838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BM25+GPT-SQ-FS</td>\n",
       "      <td>0.835660</td>\n",
       "      <td>0.398788</td>\n",
       "      <td>0.377988</td>\n",
       "      <td>0.615292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BM25+GPT-SQ-ZS</td>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.397821</td>\n",
       "      <td>0.364157</td>\n",
       "      <td>0.644563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BM25+Llama-COT</td>\n",
       "      <td>0.783448</td>\n",
       "      <td>0.285551</td>\n",
       "      <td>0.253630</td>\n",
       "      <td>0.469709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BM25+Llama-SQ-FS</td>\n",
       "      <td>0.837392</td>\n",
       "      <td>0.380488</td>\n",
       "      <td>0.368856</td>\n",
       "      <td>0.608088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BM25+Llama-SQ-ZS</td>\n",
       "      <td>0.839125</td>\n",
       "      <td>0.377338</td>\n",
       "      <td>0.346550</td>\n",
       "      <td>0.588296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BM25+Flan-COT</td>\n",
       "      <td>0.831033</td>\n",
       "      <td>0.353227</td>\n",
       "      <td>0.357316</td>\n",
       "      <td>0.593031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BM25+Flan-SQ-FS</td>\n",
       "      <td>0.833705</td>\n",
       "      <td>0.374224</td>\n",
       "      <td>0.358760</td>\n",
       "      <td>0.540278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BM25+Flan-SQ-ZS</td>\n",
       "      <td>0.824227</td>\n",
       "      <td>0.364216</td>\n",
       "      <td>0.358128</td>\n",
       "      <td>0.553245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  recall_1000  ndcg_cut_5  ndcg_cut.10  recip_rank\n",
       "0               BM25     0.825376    0.393650     0.374041    0.579877\n",
       "1     BM25_stopwords     0.834410    0.382805     0.367656    0.581239\n",
       "2           BM25_Bo1     0.819550    0.352757     0.344307    0.540783\n",
       "3           BM25+RM3     0.807109    0.339887     0.320692    0.544242\n",
       "4            BM25+KL     0.821885    0.351864     0.347616    0.549747\n",
       "5       BM25+GPT-COT     0.825698    0.320591     0.305605    0.524838\n",
       "6     BM25+GPT-SQ-FS     0.835660    0.398788     0.377988    0.615292\n",
       "7     BM25+GPT-SQ-ZS     0.829955    0.397821     0.364157    0.644563\n",
       "8     BM25+Llama-COT     0.783448    0.285551     0.253630    0.469709\n",
       "9   BM25+Llama-SQ-FS     0.837392    0.380488     0.368856    0.608088\n",
       "10  BM25+Llama-SQ-ZS     0.839125    0.377338     0.346550    0.588296\n",
       "11     BM25+Flan-COT     0.831033    0.353227     0.357316    0.593031\n",
       "12   BM25+Flan-SQ-FS     0.833705    0.374224     0.358760    0.540278\n",
       "13   BM25+Flan-SQ-ZS     0.824227    0.364216     0.358128    0.553245"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    retr_systems=[bm25, bm25_stopwords, bm25_bo1, bm25_rm3, bm25_kl, pipeline_gpt_cot, pipeline_gpt_sq_fs, pipeline_gpt_sq_zs, pipeline_llama_cot, pipeline_llama_sq_fs, pipeline_llama_sq_zs, pipeline_flan_cot, pipeline_flan_sq_fs, pipeline_flan_sq_zs],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25', 'BM25_stopwords','BM25_Bo1', 'BM25+RM3', 'BM25+KL', 'BM25+GPT-COT', 'BM25+GPT-SQ-FS', 'BM25+GPT-SQ-ZS', 'BM25+Llama-COT', 'BM25+Llama-SQ-FS', 'BM25+Llama-SQ-ZS', 'BM25+Flan-COT', 'BM25+Flan-SQ-FS', 'BM25+Flan-SQ-ZS'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:57:32.807 [main] WARN org.terrier.querying.RM1 - Did not identify any usable candidate expansion terms from docid 125137 among 6 possibilities\n",
      "14:57:32.970 [main] WARN org.terrier.querying.RM1 - Did not identify any usable candidate expansion terms from docid 116910 among 5 possibilities\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretr_systems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mbm25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25_rm3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25_kl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_gpt_cot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_gpt_sq_fs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_gpt_sq_zs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_llama_cot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_llama_sq_fs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_llama_sq_zs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_flan_cot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_flan_sq_fs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_flan_sq_zs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_qrels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+RM3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+KL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+GPT-COT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+GPT-SQ-FS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+GPT-SQ-ZS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Llama-COT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Llama-SQ-FS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Llama-SQ-ZS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Flan-COT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Flan-SQ-FS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25+Flan-SQ-ZS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecall_1000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mndcg_cut_5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mndcg_cut.10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecip_rank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/pipelines.py:471\u001b[0m, in \u001b[0;36mExperiment\u001b[0;34m(retr_systems, topics, qrels, eval_metrics, names, perquery, dataframe, batch_size, filter_by_qrels, filter_by_topics, baseline, test, correction, correction_alpha, highlight, round, verbose, save_dir, save_mode, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     save_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.res.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[0;32m--> 471\u001b[0m time, evalMeasuresDict \u001b[38;5;241m=\u001b[39m \u001b[43m_run_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackfill_qids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_topic_qids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m baseline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     evalDictsPerQ\u001b[38;5;241m.\u001b[39mappend(evalMeasuresDict)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/pipelines.py:192\u001b[0m, in \u001b[0;36m_run_and_evaluate\u001b[0;34m(system, topics, qrels, metrics, pbar, save_mode, save_file, perquery, batch_size, backfill_qids)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m#transformer, evaluate all queries at once\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     starttime \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m--> 192\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m    194\u001b[0m     runtime \u001b[38;5;241m=\u001b[39m  (endtime \u001b[38;5;241m-\u001b[39m starttime) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000.\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:335\u001b[0m, in \u001b[0;36mComposedPipeline.transform\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, topics):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 335\u001b[0m         topics \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:335\u001b[0m, in \u001b[0;36mComposedPipeline.transform\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, topics):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 335\u001b[0m         topics \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/apply_base.py:217\u001b[0m, in \u001b[0;36mApplyQueryTransformer.transform\u001b[0;34m(self, inputRes)\u001b[0m\n\u001b[1;32m    215\u001b[0m     outputRes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputRes\u001b[38;5;241m.\u001b[39mprogress_apply(fn, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     outputRes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43moutputRes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputRes\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 963\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 979\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    981\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    982\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    983\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(topic)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_query\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Wrap the new function into a PyTerrier transformer\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m pt_expand_and_filter_query \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mapply\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;28;01mlambda\u001b[39;00m topic: \u001b[43mexpand_and_filter_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Update pipelines with the new query expansion function\u001b[39;00m\n\u001b[1;32m     68\u001b[0m pipeline_gpt_cot \u001b[38;5;241m=\u001b[39m (gpt_cot \u001b[38;5;241m>>\u001b[39m pt_expand_and_filter_query) \u001b[38;5;241m>>\u001b[39m bm25\n",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m, in \u001b[0;36mexpand_and_filter_query\u001b[0;34m(topic, index, num_top_docs, num_top_terms)\u001b[0m\n\u001b[1;32m     57\u001b[0m top_docs \u001b[38;5;241m=\u001b[39m bm25_res\u001b[38;5;241m.\u001b[39mhead(num_top_docs)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocno\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Select top terms from expanded query\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m filtered_query \u001b[38;5;241m=\u001b[39m \u001b[43mselect_top_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_top_terms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_query\n",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mselect_top_terms\u001b[0;34m(expanded_query, index, top_docs, num_terms)\u001b[0m\n\u001b[1;32m     35\u001b[0m term_weights \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Get tf-idf weights for terms in top documents\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m doc_term_weights \u001b[38;5;241m=\u001b[39m \u001b[43mget_tfidf_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m expanded_query\u001b[38;5;241m.\u001b[39msplit():\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m top_docs:\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mget_tfidf_weights\u001b[0;34m(index, doc_ids)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tfidf_weights\u001b[39m(index, doc_ids):\n\u001b[1;32m     24\u001b[0m     tfidf \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mapply\u001b[38;5;241m.\u001b[39mdoc_scores(index, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_IDF\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 25\u001b[0m     tfidf_res \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_ids\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     doc_term_weights \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(doc_ids):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/apply_base.py:254\u001b[0m, in \u001b[0;36mApplyGenericTransformer.transform\u001b[0;34m(self, inputRes)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputRes):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# no batching\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputRes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# batching\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/apply.py:190\u001b[0m, in \u001b[0;36mgeneric_apply.<locals>._new_column\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_new_column\u001b[39m(df):\n\u001b[0;32m--> 190\u001b[0m     df[name] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreduce\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 963\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:969\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_series_generator\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ResType, Index]:\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m    971\u001b[0m     series_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseries_generator\n\u001b[1;32m    972\u001b[0m     res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_index\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    retr_systems=[bm25, bm25_rm3, bm25_kl, pipeline_gpt_cot, pipeline_gpt_sq_fs, pipeline_gpt_sq_zs, pipeline_llama_cot, pipeline_llama_sq_fs, pipeline_llama_sq_zs, pipeline_flan_cot, pipeline_flan_sq_fs, pipeline_flan_sq_zs],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['BM25', 'BM25+RM3', 'BM25+KL', 'BM25+GPT-COT', 'BM25+GPT-SQ-FS', 'BM25+GPT-SQ-ZS', 'BM25+Llama-COT', 'BM25+Llama-SQ-FS', 'BM25+Llama-SQ-ZS', 'BM25+Flan-COT', 'BM25+Flan-SQ-FS', 'BM25+Flan-SQ-ZS'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>recall_1000</th>\n",
       "      <th>ndcg_cut_5</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>recip_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prototype1</td>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.397286</td>\n",
       "      <td>0.369355</td>\n",
       "      <td>0.644768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prototype2</td>\n",
       "      <td>0.837392</td>\n",
       "      <td>0.384409</td>\n",
       "      <td>0.368049</td>\n",
       "      <td>0.612366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prototype3</td>\n",
       "      <td>0.819550</td>\n",
       "      <td>0.379805</td>\n",
       "      <td>0.367957</td>\n",
       "      <td>0.594244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prototype4</td>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.397821</td>\n",
       "      <td>0.364157</td>\n",
       "      <td>0.644563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prototype5</td>\n",
       "      <td>0.837392</td>\n",
       "      <td>0.380488</td>\n",
       "      <td>0.368856</td>\n",
       "      <td>0.608088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prototype6</td>\n",
       "      <td>0.819550</td>\n",
       "      <td>0.352757</td>\n",
       "      <td>0.344307</td>\n",
       "      <td>0.540783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Prototype7</td>\n",
       "      <td>0.834410</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.316102</td>\n",
       "      <td>0.530231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Prototype8</td>\n",
       "      <td>0.834410</td>\n",
       "      <td>0.335382</td>\n",
       "      <td>0.312405</td>\n",
       "      <td>0.564724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Prototype9</td>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.397286</td>\n",
       "      <td>0.369355</td>\n",
       "      <td>0.644768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  recall_1000  ndcg_cut_5  ndcg_cut.10  recip_rank\n",
       "0  Prototype1     0.829955    0.397286     0.369355    0.644768\n",
       "1  Prototype2     0.837392    0.384409     0.368049    0.612366\n",
       "2  Prototype3     0.819550    0.379805     0.367957    0.594244\n",
       "3  Prototype4     0.829955    0.397821     0.364157    0.644563\n",
       "4  Prototype5     0.837392    0.380488     0.368856    0.608088\n",
       "5  Prototype6     0.819550    0.352757     0.344307    0.540783\n",
       "6  Prototype7     0.834410    0.327682     0.316102    0.530231\n",
       "7  Prototype8     0.834410    0.335382     0.312405    0.564724\n",
       "8  Prototype9     0.829955    0.397286     0.369355    0.644768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''p1 =   pipeline_gpt_sq_zs >> bm25_stopwords\n",
    "p2 = pipeline_llama_sq_fs >> bm25_stopwords\n",
    "p3 = bm25_bo1 >> bm25_stopwords\n",
    "p4 = p1 >> bm25\n",
    "p5 = p2 >> bm25\n",
    "p6 = p3 >> bm25\n",
    "\n",
    "p7 = bm25_stopwords >> pipeline_gpt_cot\n",
    "p8 = p7 >> bm25_stopwords\n",
    "p9 = p4 >> bm25_stopwords\n",
    "\n",
    "pt.Experiment(\n",
    "    retr_systems=[p1, p2, p3, p4, p5, p6, p7, p8, p9],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    names=['Prototype1', 'Prototype2', 'Prototype3', 'Prototype4', 'Prototype5', 'Prototype6', 'Prototype7', 'Prototype8', 'Prototype9'],\n",
    "    eval_metrics=['recall_1000', 'ndcg_cut_5', 'ndcg_cut.10', 'recip_rank']\n",
    ")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
